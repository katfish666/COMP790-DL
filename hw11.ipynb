{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "Recall that for the original GAN, the discriminator $f_\\phi(x)$ is trained with the following optimization:\n",
    "\n",
    "$$\\max_\\phi p(x) \\log [f_\\phi(x)] + q_\\theta(x) \\log [1 - f_\\phi(x)]$$\n",
    "\n",
    "where $p(x)$ is the true data distribution, $q_\\theta(x)$ is the generator's learned distribution, and $f_\\phi(x) \\in [0, 1]$.\n",
    "\n",
    "1. Assuming $q_\\theta(x)$ is fixed, what is the solution to this optimization problem? \"Solution\" here means the function $f_\\phi(x)$ that maximizes the above objective. Your expression should be in terms of $p(x)$ and $q_\\theta(x)$.\n",
    "1. Assume that $q_\\theta(x) = p(x)$, i.e. the generator has learned to fit the data distribution perfectly. What is the optimal discriminator in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdfgsdfgsdfg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "^ The first problem is written by hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "Consider the following min-max optimization problem: $\\min_x \\max_y xy$.\n",
    "\n",
    "1. Is there a stationary point to the function $xy$ (i.e. a point where $\\frac{\\partial xy}{\\partial x} = 0$ and $\\frac{\\partial xy}{\\partial y} = 0$)? If so, what is it?\n",
    "\n",
    "1. Consider tackling this optimization problem with alternating gradient descent (i.e. alternatingly minimizing with respect to $x$ and maximizing with respect to $y$ using gradient descent). Assume that the learning rate is the same for both steps and that optimization begins from the point $x = 1, y = 1$. Will optimization reach a stationary point? Feel free to support your answer theoretically and/or empirically (i.e. by implementing it in code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "The code below is taken from [the example in the textbook](http://d2l.ai/chapter_generative-adversarial-networks/gan.html). I've modified the example so that the true data distribution is a mixture of two Gaussians rather than a single Gaussian. Feel free to use the Pytorch code in this problem instead, but make sure you keep my modification to the data distribution.\n",
    "\n",
    "The code below uses the original GAN loss. A somewhat popular alternative loss function is the [\"Least-Squares GAN (LS-GAN)\"](https://arxiv.org/pdf/1611.04076.pdf), which is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Modify the code so that it uses the LS-GAN objective instead of the original GAN objective. Does it converge to a different solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> Need to change the below code to pytorch ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'd2l'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d26a8ffeee26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0md2l\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'd2l'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Do not change this part! It is different from the example in the textbook.\n",
    "X = np.random.normal(0.0, 1, (1000, 2))\n",
    "A = np.array([[1, 2], [-0.1, 0.5]])\n",
    "b = np.array([1, 2])\n",
    "data = np.concatenate([np.dot(X, A) + b, np.random.normal(0.0, 0.5, (100, 2)) + np.array([0, 7])], 0)\n",
    "data = torch.tensor(data)\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "data_iter = d2l.load_array((data,), batch_size)\n",
    "\n",
    "## generator\n",
    "net_G = nn.Sequential(nn.Linear(2, 2))\n",
    "\n",
    "## discriminator\n",
    "net_D = nn.Sequential(nn.Linear(2, 5), nn.Tanh(), nn.Linear(5, 3), nn.Tanh(), nn.Linear(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_D(X, Z, net_D, net_G, loss, trainer_D):\n",
    "    \"\"\"Update discriminator.\"\"\"\n",
    "    batch_size = X.shape[0]\n",
    "    ones = torch.ones((batch_size,), device=X.device)\n",
    "    zeros = torch.zeros((batch_size,), device=X.device)\n",
    "    trainer_D.zero_grad()\n",
    "    real_Y = net_D(X)\n",
    "    fake_X = net_G(Z)\n",
    "    # Do not need to compute gradient for `net_G`, detach it from\n",
    "    # computing gradients.\n",
    "    fake_Y = net_D(fake_X.detach())\n",
    "    loss_D = (loss(real_Y, ones.reshape(real_Y.shape)) +\n",
    "              loss(fake_Y, zeros.reshape(fake_Y.shape))) / 2\n",
    "    loss_D.backward()\n",
    "    trainer_D.step()\n",
    "    return loss_D"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NEXT: copy update G ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d92dfe623226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/d2l/torch.py\u001b[0m in \u001b[0;36mload_array\u001b[0;34m(data_arrays, batch_size, is_train)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;34m\"\"\"Construct a PyTorch data iterator.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "# batch_size = 8\n",
    "# data_iter = d2l.load_array((data,), batch_size)\n",
    "\n",
    "# net_G = nn.Sequential()\n",
    "# net_G.add(nn.Dense(2))\n",
    "\n",
    "# net_D = nn.Sequential()\n",
    "# net_D.add(nn.Dense(5, activation='tanh'), nn.Dense(3, activation='tanh'),\n",
    "#           nn.Dense(1))\n",
    "\n",
    "# def update_D(X, Z, net_D, net_G, loss, trainer_D):\n",
    "#     \"\"\"Update discriminator.\"\"\"\n",
    "#     batch_size = X.shape[0]\n",
    "#     ones = np.ones((batch_size,), ctx=X.ctx)\n",
    "#     zeros = np.zeros((batch_size,), ctx=X.ctx)\n",
    "#     with autograd.record():\n",
    "#         real_Y = net_D(X)\n",
    "#         fake_X = net_G(Z)\n",
    "#         # Do not need to compute gradient for `net_G`, detach it from\n",
    "#         # computing gradients.\n",
    "#         fake_Y = net_D(fake_X.detach())\n",
    "#         loss_D = (loss(real_Y, ones) + loss(fake_Y, zeros)) / 2\n",
    "#     loss_D.backward()\n",
    "#     trainer_D.step(batch_size)\n",
    "#     return float(loss_D.sum())\n",
    "\n",
    "def update_G(Z, net_D, net_G, loss, trainer_G):\n",
    "    \"\"\"Update generator.\"\"\"\n",
    "    batch_size = Z.shape[0]\n",
    "    ones = np.ones((batch_size,), ctx=Z.ctx)\n",
    "    with autograd.record():\n",
    "        # We could reuse `fake_X` from `update_D` to save computation\n",
    "        fake_X = net_G(Z)\n",
    "        # Recomputing `fake_Y` is needed since `net_D` is changed\n",
    "        fake_Y = net_D(fake_X)\n",
    "        loss_G = loss(fake_Y, ones)\n",
    "    loss_G.backward()\n",
    "    trainer_G.step(batch_size)\n",
    "    return float(loss_G.sum())\n",
    "\n",
    "def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    net_D.initialize(init=init.Normal(0.02), force_reinit=True)\n",
    "    net_G.initialize(init=init.Normal(0.02), force_reinit=True)\n",
    "    trainer_D = gluon.Trainer(net_D.collect_params(), 'adam',\n",
    "                              {'learning_rate': lr_D})\n",
    "    trainer_G = gluon.Trainer(net_G.collect_params(), 'adam',\n",
    "                              {'learning_rate': lr_G})\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n",
    "                            legend=['discriminator', 'generator'])\n",
    "    animator.fig.subplots_adjust(hspace=0.3)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train one epoch\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n",
    "        for X in data_iter:\n",
    "            batch_size = X.shape[0]\n",
    "            Z = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            metric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),\n",
    "                       update_G(Z, net_D, net_G, loss, trainer_G), batch_size)\n",
    "        # Visualize generated examples\n",
    "        Z = np.random.normal(0, 1, size=(1000, latent_dim))\n",
    "        fake_X = net_G(Z).asnumpy()\n",
    "        animator.axes[1].cla()\n",
    "        animator.axes[1].scatter(data[:, 0], data[:, 1], alpha=0.2)\n",
    "        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1], alpha=0.2)\n",
    "        animator.axes[1].legend(['real', 'generated'])\n",
    "        # Show the losses\n",
    "        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n",
    "        animator.add(epoch + 1, (loss_D, loss_G))\n",
    "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "          f'{metric[2] / timer.stop():.1f} examples/sec')\n",
    "    \n",
    "lr_D, lr_G, latent_dim, num_epochs = 0.05, 0.005, 2, 20\n",
    "train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
